{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resnet from UVic playground /models dir\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "\n",
    "\n",
    "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
    "           'resnet152']\n",
    "\n",
    "\n",
    "model_urls = {\n",
    "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
    "}\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = conv1x1(inplanes, planes)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = conv3x3(planes, planes, stride)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = conv1x1(planes, planes * self.expansion)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_input_channels=2, num_classes=1000, zero_init_residual=False):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.inplanes = 64\n",
    "        self.conv1 = nn.Conv2d(num_input_channels, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def resnet18(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet18']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet34(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-34 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet34']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet50(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-50 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet101(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-101 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet101']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet152(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-152 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet152']))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Author: Wojciech Fedorko\n",
    "Collaborators: Julian Ding, Abhishek Kajal\n",
    "'''\n",
    "\n",
    "import copy # Currently unused\n",
    "import re # Currently unused\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "#import torch.nn as nn\n",
    "from torch.autograd import Variable # Currently unused\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np # Currently unused\n",
    "import time\n",
    "\n",
    "from statistics import mean # Currently unused\n",
    "\n",
    "import shutil # Currently unused\n",
    "import os # Currently unused\n",
    "\n",
    "import sklearn # Currently unused\n",
    "from sklearn.metrics import roc_curve # Currently unused\n",
    "\n",
    "\n",
    "from iotools.data_handling import WCH5Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "class Engine:\n",
    "    \"\"\"The training engine \n",
    "    \n",
    "    Performs training and evaluation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, config):\n",
    "        self.model = model\n",
    "\n",
    "        if config.gpu:\n",
    "            print(\"requesting gpu \")\n",
    "            print(\"gpu list: \")\n",
    "            print(config.gpu_list)\n",
    "            self.devids = [\"cuda:{0}\".format(x) for x in config.gpu_list]\n",
    "\n",
    "            print(\"main gpu: \"+self.devids[0])\n",
    "            if torch.cuda.is_available():\n",
    "                self.device = torch.device(self.devids[0])\n",
    "                if len(self.devids) > 1:\n",
    "                    print(\"using DataParallel on these devices: {}\".format(self.devids))\n",
    "                    self.model = nn.DataParallel(self.model, device_ids=config.gpu_list, dim=0)\n",
    "\n",
    "                print(\"cuda is available\")\n",
    "            else:\n",
    "                self.device=torch.device(\"cpu\")\n",
    "                print(\"cuda is not available\")\n",
    "        else:\n",
    "            print(\"will not use gpu\")\n",
    "            self.device=torch.device(\"cpu\")\n",
    "\n",
    "        print(self.device)\n",
    "\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.model.parameters(),eps=1e-3)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        #placeholders for data and labels\n",
    "        self.data=None\n",
    "        self.labels=None\n",
    "        self.iteration=None\n",
    "\n",
    "        # NOTE: The functionality of this block is coupled to the implementation of WCH5Dataset in the iotools module\n",
    "        self.dset=WCH5Dataset(config.path, config.val_split, config.test_split)\n",
    "\n",
    "        self.train_iter=DataLoader(self.dset,\n",
    "                                   batch_size=config.batch_size_train,\n",
    "                                   shuffle=True,\n",
    "                                   sampler=SubsetRandomSampler(self.dset.train_indices))\n",
    "        \n",
    "        self.val_iter=DataLoader(self.dset,\n",
    "                                 batch_size=config.batch_size_val,\n",
    "                                 shuffle=True,\n",
    "                                 sampler=SubsetRandomSampler(self.dset.val_indices))\n",
    "        \n",
    "        self.test_iter=DataLoader(self.dset,\n",
    "                                  batch_size=config.batch_size_test,\n",
    "                                  shuffle=True,\n",
    "                                  sampler=SubsetRandomSampler(self.dset.test_indices))\n",
    "\n",
    "        \n",
    "\n",
    "        self.dirpath=config.save_path\n",
    "        \n",
    "        self.data_description=config.data_description\n",
    "\n",
    "\n",
    "        \n",
    "        try:\n",
    "            os.stat(self.dirpath)\n",
    "        except:\n",
    "            print(\"making a directory for model data: {}\".format(self.dirpath))\n",
    "            os.mkdir(self.dirpath)\n",
    "\n",
    "        #add the path for the data type to the dirpath\n",
    "        self.start_time_str = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        self.dirpath=self.dirpath+'/'+self.data_description + \"/\" + self.start_time_str\n",
    "\n",
    "        try:\n",
    "            os.stat(self.dirpath)\n",
    "        except:\n",
    "            print(\"making a directory for model data for data prepared as: {}\".format(self.data_description))\n",
    "            os.makedirs(self.dirpath,exist_ok=True)\n",
    "\n",
    "        self.config=config\n",
    "\n",
    "\n",
    "    def forward(self,train=True):\n",
    "        \"\"\"\n",
    "        Args: self should have attributes, model, criterion, softmax, data, label\n",
    "        Returns: a dictionary of predicted labels, softmax, loss, and accuracy\n",
    "        \"\"\"\n",
    "        with torch.set_grad_enabled(train):\n",
    "            # Prediction\n",
    "            #print(\"this is the data size before permuting: {}\".format(data.size()))\n",
    "            self.data = self.data.permute(0,3,1,2)\n",
    "            #print(\"this is the data size after permuting: {}\".format(data.size()))\n",
    "            prediction = self.model(self.data)\n",
    "            # Training\n",
    "            loss,acc=-1,-1 # NOTE: What is acc supposed to do? It's never used....\n",
    "            \n",
    "            loss = self.criterion(prediction,self.label)\n",
    "            self.loss = loss\n",
    "            \n",
    "            softmax    = self.softmax(prediction).cpu().detach().numpy()\n",
    "            prediction = torch.argmax(prediction,dim=-1)\n",
    "            accuracy   = (prediction == self.label).sum().item() / float(prediction.nelement())        \n",
    "            prediction = prediction.cpu().detach().numpy()\n",
    "        \n",
    "        return {'prediction' : prediction,\n",
    "                'softmax'    : softmax,\n",
    "                'loss'       : loss.cpu().detach().item(),\n",
    "                'accuracy'   : accuracy}\n",
    "\n",
    "    def backward(self):\n",
    "        self.opt.zero_grad()  # Reset gradients accumulation\n",
    "        self.loss.backward()\n",
    "        self.opt.step()\n",
    "        \n",
    "    # ========================================================================\n",
    "    def train(self, epochs, report_interval=10, valid_interval=100):\n",
    "        # CODE BELOW COPY-PASTED FROM [HKML CNN Image Classification.ipynb]\n",
    "        # (variable names changed to match new Engine architecture. Added comments and minor debugging)\n",
    "        \n",
    "        # Prepare attributes for data logging\n",
    "        self.train_log, self.test_log = CSVData(self.dirpath+'/log_train.csv'), CSVData(self.dirpath+'/log_test.csv')\n",
    "        # Set neural net to training mode\n",
    "        self.model.train()\n",
    "        # Initialize epoch counter\n",
    "        epoch = 0.\n",
    "        # Initialize iteration counter\n",
    "        iteration = 0\n",
    "        # Training loop\n",
    "        while (int(epoch+0.5) < epochs):\n",
    "            from notebook_utils import progress_bar, CSVData\n",
    "            from IPython.display import display\n",
    "            print('Epoch',int(epoch+0.5),'Starting @',time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "            # Create a progress bar for this epoch\n",
    "            progress = display(progress_bar(0,len(self.train_iter)),display_id=True)\n",
    "            # Loop over data samples and into the network forward function\n",
    "            for i, data in enumerate(self.train_iter):\n",
    "                # Data and label\n",
    "                self.data, self.label = data[0:2]\n",
    "                # Call forward: make a prediction & measure the average error\n",
    "                res = self.forward(True)\n",
    "                # Call backward: backpropagate error and update weights\n",
    "                self.backward()\n",
    "                # Epoch update\n",
    "                epoch += 1./len(self.train_iter)\n",
    "                iteration += 1\n",
    "                \n",
    "                #\n",
    "                # Log/Report\n",
    "                #\n",
    "                # Record the current performance on train set\n",
    "                self.train_log.record(['iteration','epoch','accuracy','loss'],[iteration,epoch,res['accuracy'],res['loss']])\n",
    "                self.train_log.write()\n",
    "                # once in a while, report\n",
    "                if i==0 or (i+1)%report_interval == 0:\n",
    "                    message = '... Iteration %d ... Epoch %1.2f ... Loss %1.3f ... Accuracy %1.3f' % (iteration,epoch,res['loss'],res['accuracy'])\n",
    "                    progress.update(progress_bar((i+1),len(self.train_iter),message))\n",
    "                # more rarely, run validation\n",
    "                if (i+1)%valid_interval == 0:\n",
    "                    with torch.no_grad():\n",
    "                        self.model.eval()\n",
    "                        test_data = next(iter(self.test_iter))\n",
    "                        self.data, self.label = test_data[0:2]\n",
    "                        res = self.forward(False)\n",
    "                        self.test_log.record(['iteration','epoch','accuracy','loss'],[iteration,epoch,res['accuracy'],res['loss']])\n",
    "                        self.test_log.write()\n",
    "                    self.model.train()\n",
    "                if epoch >= epochs:\n",
    "                    break\n",
    "            message = '... Iteration %d ... Epoch %1.2f ... Loss %1.3f ... Accuracy %1.3f' % (iteration,epoch,res['loss'],res['accuracy'])\n",
    "            progress.update(progress_bar((i+1),len(self.train_iter),message))\n",
    "        \n",
    "        self.test_log.close()\n",
    "        self.train_log.close()\n",
    "    \n",
    "    # ========================================================================\n",
    "\n",
    "    def save_state(self, prefix='./snapshot'):\n",
    "        # Output file name\n",
    "        #filename = '%s-%d.ckpt' % (prefix, self.iteration)\n",
    "        filename = '%s.ckpt' % (prefix)\n",
    "    \n",
    "        # Save parameters\n",
    "        # 0+1) iteration counter + optimizer state => in case we want to \"continue training\" later\n",
    "        # 2) network weight\n",
    "        torch.save({\n",
    "            'global_step': self.iteration,\n",
    "            'optimizer': self.opt.state_dict(),\n",
    "            'state_dict': self.model.state_dict()\n",
    "        }, filename)\n",
    "        return filename\n",
    "\n",
    "    def restore_state(self,weight_file):\n",
    "        # Open a file in read-binary mode\n",
    "        with open(weight_file, 'rb') as f:\n",
    "            # torch interprets the file, then we can access using string keys\n",
    "            checkpoint = torch.load(f)\n",
    "            # load network weights\n",
    "            self.net.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "            # if optim is provided, load the state of the optim\n",
    "            if self.opt is not None:\n",
    "                self.opt.load_state_dict(checkpoint['optimizer'])\n",
    "            # load iteration count\n",
    "            self.iteration = checkpoint['global_step']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preamble that would be done in __main__ to generate config object\n",
    "\n",
    "model = resnet18(num_input_channels=38)\n",
    "\n",
    "class CONFIG:\n",
    "    pass\n",
    "\n",
    "config = CONFIG()\n",
    "config.gpu = False\n",
    "\n",
    "\n",
    "#config.path = '/project/rpp-tanaka-ab/machine_learning/data/IWCDmPMT/varyE/merged_IWCDmPMT_varyE.h5'\n",
    "config.path = \n",
    "config.val_split = 0.33\n",
    "config.test_split = 0.33\n",
    "\n",
    "config.batch_size_train = 20\n",
    "config.batch_size_val = 100\n",
    "config.batch_size_test =100\n",
    "\n",
    "config.save_path = 'save_path'\n",
    "config.data_description = 'basic Engine test data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will not use gpu\n",
      "cpu\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = '/project/rpp-tanaka-ab/machine_learning/data/IWCDmPMT/varyE/merged_IWCDmPMT_varyE.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-39fd81e30f6c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Test neural net\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mengine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEngine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-595a234a4765>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model, config)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;31m# NOTE: The functionality of this block is coupled to the implementation of WCH5Dataset in the iotools module\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mWCH5Dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval_split\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_split\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         self.train_iter=DataLoader(self.dset,\n",
      "\u001b[1;32m~\\Documents\\GitHub\\CNN\\CNN\\training_utils\\iotools\\data_handling.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path, val_split, test_split, transform, reduced_dataset_size, seed)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mhdf5_event_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"event_data\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mhdf5_labels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"labels\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[0;32m    310\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m                 \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m             \u001b[0mflags\u001b[0m \u001b[1;33m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'r+'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to open file (unable to open file: name = '/project/rpp-tanaka-ab/machine_learning/data/IWCDmPMT/varyE/merged_IWCDmPMT_varyE.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "# Test neural net\n",
    "\n",
    "engine = Engine(model, config)\n",
    "engine.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
